---
title: LLM as Reranker
description: 'Flexible reranking using LLMs'
---

LLM-based reranker provides maximum flexibility by using any Large Language Model to score document relevance. This approach allows for custom prompts and domain-specific scoring logic.

## Supported LLM Providers

Any LLM provider supported by Mem0 can be used for reranking:

- **OpenAI**: GPT-4, GPT-3.5-turbo, etc.
- **Anthropic**: Claude models
- **Together**: Open-source models
- **Groq**: Fast inference
- **Ollama**: Local models
- And more...

## Configuration

```ts
import { Memory } from "mem0ai/oss";

const memory = new Memory({
  vectorStore: {
    provider: "chroma",
    config: { collectionName: "my_memories", path: "./chroma_db" },
  },
  llm: {
    provider: "openai",
    config: { model: "gpt-4o-mini" },
  },
  reranker: {
    provider: "llm",
    config: {
      model: "gpt-4o-mini",
      provider: "openai",
      apiKey: process.env.OPENAI_API_KEY,
      topK: 5,
      temperature: 0.0,
    },
  },
});
```

## Custom Scoring Prompt

Provide a custom prompt for relevance scoring:

```ts
const memory = new Memory({
  reranker: {
    provider: "llm",
    config: {
      model: "gpt-4o-mini",
      provider: "openai",
      temperature: 0.0,
      scoringPrompt: `You are a relevance scoring assistant. Rate how well this document answers the query.

Query: "{query}"
Document: "{document}"

Score from 0.0 to 1.0 where:
- 1.0: Perfect match, directly answers the query
- 0.8-0.9: Highly relevant, good match
- 0.6-0.7: Moderately relevant, partial match
- 0.4-0.5: Slightly relevant, limited useful information
- 0.0-0.3: Not relevant or no useful information

Provide only a single numerical score between 0.0 and 1.0.`,
    },
  },
});
```

## Usage Example

```ts
import { Memory } from "mem0ai/oss";

const memory = new Memory({
  vectorStore: { provider: "chroma", config: { collectionName: "my_memories", path: "./chroma_db" } },
  llm: { provider: "openai", config: { model: "gpt-4o-mini" } },
  reranker: { provider: "llm", config: { model: "gpt-4o-mini", provider: "openai", temperature: 0.0 } },
});

await memory.add([
  { role: "user", content: "I'm learning Python programming" },
  { role: "user", content: "I find object-oriented programming challenging" },
  { role: "user", content: "I love hiking in national parks" },
], { userId: "david" });

const results = await memory.search("What programming topics is the user studying?", { userId: "david" });
results.results.forEach((r: any) => {
  console.log(`Memory: ${r.memory}, Score: ${r.score.toFixed(3)}`);
});
```

## Multiple LLM Providers

```ts
// Using Anthropic Claude
const memory = new Memory({
  reranker: {
    provider: "llm",
    config: { model: "claude-3-haiku-20240307", provider: "anthropic", temperature: 0.0 },
  },
});

// Using local Ollama model
const memoryLocal = new Memory({
  reranker: {
    provider: "llm",
    config: { model: "llama2:7b", provider: "ollama", temperature: 0.0 },
  },
});
```

## Configuration Parameters

| Parameter | Description | Type | Default |
|-----------|-------------|------|---------|
| `model` | LLM model to use for scoring | `string` | `"gpt-4o-mini"` |
| `provider` | LLM provider name | `string` | `"openai"` |
| `apiKey` | API key for the LLM provider | `string` | `undefined` |
| `topK` | Maximum documents to return | `number` | `undefined` |
| `temperature` | Temperature for LLM generation | `number` | `0.0` |
| `maxTokens` | Maximum tokens for LLM response | `number` | `100` |
| `scoringPrompt` | Custom prompt template | `string` | Default prompt |

## Advantages

- **Maximum Flexibility**: Custom prompts for any use case
- **Domain Expertise**: Leverage LLM knowledge for specialized domains
- **Interpretability**: Understand scoring through prompt engineering
- **Multi-criteria**: Score based on multiple relevance factors

## Considerations

- **Latency**: Higher latency than specialized rerankers
- **Cost**: LLM API costs per reranking operation
- **Consistency**: May have slight variations in scoring
- **Prompt Engineering**: Requires careful prompt design

## Best Practices

1. **Temperature**: Use 0.0 for consistent scoring
2. **Prompt Design**: Be specific about scoring criteria
3. **Token Efficiency**: Keep prompts concise to reduce costs
4. **Caching**: Cache results for repeated queries when possible
5. **Fallback**: Handle API errors gracefully
