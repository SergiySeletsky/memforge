You can use embedding models from LM Studio to run Mem0 locally.

### Usage

```ts
import { Memory } from "mem0ai/oss";

const memory = new Memory({
  embedder: {
    provider: "lmstudio",
    config: {
      model: "nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.f16.gguf",
      lmstudioBaseUrl: "http://localhost:1234/v1",
    },
  },
});

const messages = [
  { role: "user", content: "I'm planning to watch a movie tonight. Any recommendations?" },
  { role: "assistant", content: "How about thriller movies? They can be quite engaging." },
  { role: "user", content: "I'm not a big fan of thriller movies but I love sci-fi movies." },
  { role: "assistant", content: "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future." },
];

await memory.add(messages, { userId: "john" });
```

### Config

Here are the parameters available for configuring the LM Studio embedder:

| Parameter | Description | Default Value |
| --- | --- | --- |
| `model` | The name of the LM Studio model to use | `nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.f16.gguf` |
| `embeddingDims` | Dimensions of the embedding model | `1536` |
| `lmstudioBaseUrl` | Base URL for LM Studio connection | `http://localhost:1234/v1` |
