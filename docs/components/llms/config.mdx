---
title: Configurations
---

## How to define configurations?

The `config` is defined as a TypeScript object with these keys:
- `llm`: Specifies the LLM provider and its configuration (required)
  - `provider`: The name of the LLM (e.g., "openai", "groq")
  - `config`: A nested object containing provider-specific settings
- `embedder`: Specifies the embedder provider and its configuration (optional)
- `vectorStore`: Specifies the vector store provider and its configuration (optional)
- `historyDbPath`: Path to the history database file (optional)

### Config Values Precedence

Config values are applied in the following order of precedence (from highest to lowest):

1. Values explicitly set in the `config` object/dictionary
2. Environment variables (e.g., `OPENAI_API_KEY`, `OPENAI_BASE_URL`)
3. Default values defined in the LLM implementation

This means that values specified in the `config` will override corresponding environment variables, which in turn override default values.

## How to Use Config

Here's a general example of how to use the config with Mem0:

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

// Minimal configuration with just the LLM settings
const config = {
  llm: {
    provider: 'your_chosen_provider',
    config: {
      // Provider-specific settings go here
    }
  }
};

const memory = new Memory(config);
await memory.add("Your text here", { userId: "user123", metadata: { category: "example" } });
```

## Why is Config Needed?

Config is essential for:
1. Specifying which LLM to use.
2. Providing necessary connection details (e.g., model, api_key, temperature).
3. Ensuring proper initialization and connection to your chosen LLM.

## Master List of All Params in Config

Here's a comprehensive list of all parameters that can be used across different LLMs:

| Parameter            | Description                                   | Provider          |
|----------------------|-----------------------------------------------|-------------------|
| `model`              | Embedding model to use                        | All               |
| `temperature`        | Temperature of the model                      | All               |
| `apiKey`             | API key to use                                | All               |
| `maxTokens`          | Tokens to generate                            | All               |
| `topP`               | Probability threshold for nucleus sampling    | All               |
| `topK`               | Number of highest probability tokens to keep  | All               |
| `openaiBaseUrl`      | Base URL for OpenAI API                       | OpenAI            |

## Supported LLMs

For detailed information on configuring specific LLMs, please visit the [LLMs](./models) section. There you'll find information for each supported LLM with provider-specific usage examples and configuration details.
