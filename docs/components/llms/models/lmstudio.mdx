---
title: LM Studio
---

[LM Studio](https://lmstudio.ai/) lets you run local LLMs with an OpenAI-compatible API. Start the local server from the "Server" tab (default URL: `http://localhost:1234/v1`).

## Usage

```ts
import { Memory } from "mem0ai/oss";

const memory = new Memory({
  llm: {
    provider: "lmstudio",
    config: {
      model: "lmstudio-community/Meta-Llama-3.1-70B-Instruct-GGUF/Meta-Llama-3.1-70B-Instruct-IQ2_M.gguf",
      temperature: 0.2,
      maxTokens: 2000,
      lmstudioBaseUrl: "http://localhost:1234/v1",
    },
  },
});

const messages = [
  { role: "user", content: "I'm planning to watch a movie tonight. Any recommendations?" },
  { role: "assistant", content: "How about thriller movies? They can be quite engaging." },
  { role: "user", content: "I'm not a big fan of thriller movies but I love sci-fi movies." },
  { role: "assistant", content: "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future." },
];

await memory.add(messages, { userId: "alice", metadata: { category: "movies" } });
```

## Running Completely Locally

Use LM Studio for both LLM and embedding to run Mem0 entirely without external API keys:

```ts
import { Memory } from "mem0ai/oss";

const memory = new Memory({
  llm: {
    provider: "lmstudio",
    config: { lmstudioBaseUrl: "http://localhost:1234/v1" },
  },
  embedder: {
    provider: "lmstudio",
    config: { lmstudioBaseUrl: "http://localhost:1234/v1" },
  },
});

await memory.add(messages, { userId: "alice" });
```

<Note>
  When using LM Studio for both LLM and embedding, make sure you have:
  1. An LLM model loaded for generating responses
  2. An embedding model loaded for vector embeddings
  3. The server enabled with the correct endpoints accessible
</Note>

## Config

All available parameters for the `lmstudio` config are present in [Master List of All Params in Config](../config).
