---
title: DeepSeek
---

To use DeepSeek LLM models, set the `DEEPSEEK_API_KEY` environment variable. You can also optionally set `DEEPSEEK_API_BASE` if you need a different API endpoint (defaults to `https://api.deepseek.com`).

## Usage

```ts
import { Memory } from "mem0ai/oss";

const memory = new Memory({
  llm: {
    provider: "deepseek",
    config: {
      apiKey: process.env.DEEPSEEK_API_KEY,
      model: "deepseek-chat",
      temperature: 0.2,
      maxTokens: 2000,
      topP: 1.0,
    },
  },
});

const messages = [
  { role: "user", content: "I'm planning to watch a movie tonight. Any recommendations?" },
  { role: "assistant", content: "How about thriller movies? They can be quite engaging." },
  { role: "user", content: "I'm not a big fan of thriller movies but I love sci-fi movies." },
  { role: "assistant", content: "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future." },
];

await memory.add(messages, { userId: "alice", metadata: { category: "movies" } });
```

You can also configure a custom API base URL:

```ts
const memory = new Memory({
  llm: {
    provider: "deepseek",
    config: {
      apiKey: process.env.DEEPSEEK_API_KEY,
      model: "deepseek-chat",
      deepseekBaseUrl: "https://your-custom-endpoint.com",
    },
  },
});
```

## Config

All available parameters for the `deepseek` config are present in [Master List of All Params in Config](../config).
